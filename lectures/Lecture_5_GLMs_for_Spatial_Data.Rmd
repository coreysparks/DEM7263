---
title: "DEM 7263 - Generalized Linear Models for Spatial Count data"
author: "Corey S. Sparks, Ph.D."
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_document:
    df_print: paged
    fig_height: 7
    fig_width: 8
    toc: yes
    toc_float: yes
    code_download: true
---
# Overview

This lesson will cover the analysis of aggregate data in spatial demography. Often times, data are available for areas rather than individuals, and generally, these data are for counts of observations within these areas, or rates of occurrence for a certain type of event. When data are available in this form, the linear regression model may not be an optimal choice for analyzing the data. Instead of the linear model, a **_generalized linear model_** may be preferred. These types of models allow for different types of statistical distributions to be used in the regression context. 


### Objectives
This lesson has three objectives:

 1) Define and illustrate the use of the Generalized Linear regression model for aggregate count data

 2) Define and illustrate epidemiological analysis using the relative risk concept
 
 3) Describe the Area Health Resources File (AHRF) as a source of demographic and health data that is freely available.
 




```{r setup, cache = F, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set( fig.width=9, fig.height=8)
```

# Defining aggregate count data. 

## Aggregate data
Data on areal units are very common in demographic research, and prior to the wide scale availability of individual-level microdata from surveys, were the standard data source available for demographic analysis. These are referred to as **_aggregate data_** because they represent the total number of events of a given type. For instance, the total number of deaths or total number of births. These are often not measured separately by other demographic characteristics, such as age, race or sex, although certain data sources do provide aggregate measures for specific demographic groups.

Typically when we get data with a spatial component, they are aggregated to some geography examples of these are:
 
  * Census tracts, Census blocks, counties, and ZIP codes
 
Data measured on these areas are typically either raw counts, such as a count of deaths, crimes, births, people with some characteristic (people with incomes below the poverty line). These form the numerator for any rate that we might calculate. For the denominator, we hopefully have a population at risk of experiencing the event of interest, such as the total population for a crude death rate, or the number of women between a certain age, for an age specific fertility rate, for instance. 

Examples of places where aggregate data can be obtained are the US Census Bureau's American Community Survey summary file, which we have been using throughout this course, and the [CDC Wonder](https://wonder.cdc.gov/) data portal, which allows for aggregate data on birth and death rates to be extracted. 
   
### Describe the general linear model for count data

Up until now, we have been relying on linear statistical models which assumed the Normal distribution for our outcomes. A broader class of regression models, are [**_Generalized Linear Models_**](https://en.wikipedia.org/wiki/Generalized_linear_model), or GLMs, which allow for linear regression for outcomes that are not assumed to come from a Normal distribution. 

GLMs are a class of statistical models that link the mean of the specified distribution to a linear combination of covariates by some form of **_link function_**. For example, the Normal distribution has the mean, $\mu$, which is typically estimated using the **_linear mean function_** :

$$\mu = \beta_0 + \beta_1 x_1$$
Which describes the line that estimates the mean of the outcome variable as a linear function of the predictor variable $x_1$. This model uses an **_identity link_** meaning there is no transformation of the linear mean function as it is connected to the mean of the outcome. 

This can be written as:

$$g(u) = g(E(Y)) = \beta_0 + \beta_1 x_1$$

Where $g()$ is the link function, linking the mean of the Normal distribution to the linear mean function of the model.

The linear model is appropriate for the Normal distribution, because this distribution can take any value from $- \infty$  to $\infty$. Other distributions do not have this wide range, so transformations of the linear mean function must be used so that the linear model remains on the scale of the data. 


### Other common distributions for aggregate data 

##Poisson disribution for counts or rates
A model that is used commonly for the analysis of count data, meaning integer data, such as the number of deaths, number of crimes or number of cases of a disease, is the **_Poisson Distribution_**. 

For the Poisson model, you observe a count of events in a small area or time period (crimes, deaths, etc). Unlike the Normal distribution, the Poisson is defined strictly for positive integer values.

The Poisson distribution has a single parameter, the mean or $\lambda$. When we construct a model for the mean of the Poisson model, we use a natural logarithm link function. This model is referred to as a type of **_log-linear model_**. 

The mean count of the event, $\lambda$ is linked to the linear mean function through the natural log link:

$$ln(\lambda) = \beta_0 + \beta_1 x_1$$

and using the relationship between exponents and natural logarithms, the mean is:
$$\lambda = \text{exp}( \beta_0 + \beta_1 x_1)$$

which ensures that the linear mean function is always positive, matching the range of the Poisson count data. 


## Poisson distribution modeling
The mean of the Poisson distribution, $(\lambda)$, is really the average count for the outcome ($y$). We have several ways of modeling the Poisson count: 

  * _Pure count model_ If each area has the same risk set, or population size, then we can model the mean as-is. This would lead to a model that looks like:
  
  $$log(y)= \beta_0 + \beta_1 x_1$$
  
When we see the $\beta_1$ parameter in this model in computer output, it is on the log-scale, since that is the scale of the outcome for the Poisson model. In order to interpret the $\beta_1$, we have to **_exponentiate_** it. When we do this, the parameter is interpreted as the percentage change in the mean of the outcome, for a 1 unit change in $x_1$. For instance if we estimate a model and see in the output that $\beta_1 = \text{.025}$, then $\exp(\beta_1) = \text{exp}(\text{.025}) = \text{1.025}$, or for a 1 unit increase in $x_1$, the mean of $y$ increases by 1.025. So if the mean of $y$ is 10, when $x_1$ = 0, then the mean is $10*(1.025*1)$ or $10.25$  when $x_1$ = 1.  

  * _Rate model_ The second type of modeling strategy is a model for a rate of occurrence. This model includes an **_offset term_** in the model to incorporate unequal population sizes, this is the most common way the data are analyzed in demographic research. This offset term can be thought of as the numerator for the rate, and we can show how it is put into the model. 
  
  If $n$ is the population size for each place, then, we want to do a regression on the rate of occurrence of our outcome. The rate is typically expressed as a proportion, or probability $rate = \frac{y}{n}$:

$$log(y/n)= \beta_0 + \beta_1 x_1$$
$$log(y) - log(n)= \beta_0 + \beta_1 x_1$$

$$log(y)= \beta_0 + \beta_1 x_1 + log(n)$$

Similar to the example from before, when interpreting the effect of $\beta_1$ in this model, we also have to exponentiate it. In this case, the interpretation would not be related to the overall count, but to the rate of occurrence. So, if as before, the $\beta_1 = \text{.025}$, then $\exp(\beta_1) = \text{exp}(\text{.025}) = \text{1.025}$, or for a 1 unit increase in $x_1$, the **_rate_** of occurrence of $y$ increases by 1.025. If, in this case the average mortality rate was $10/500 = 0.02$ when $x_1$ = 0, then the rate is $0.02*(1.025*1)$ or $0.0205$  when $x_1$ = 1.  

### Relative risk analysis 

The third type of model for the Poisson distribution focuses on the idea of the relative risk of an event, and uses the **_Standardized risk ratio_** as its currency. 

  * The _Standardized risk ratio_ incorporates differential exposure due to population size as an **_expected count_** of the outcome in the offset term, and are typically seen in epidemiological studies. The expected count $E$, incorporates the different population sizes of each area by estimating the number of events that should occur, if the area followed a given rate of occurrence. 

The expected count is calculated by multiplying the average rate of occurrence, $r$, by the population size, $n$:  $E_i = r * n_i$, where $r = \frac{\sum y_i}{\sum n_i}$, is the overall rate in the population. This method is commonly referred to as **_internal standaridization_** because we are using the data at hand to estimate the overall rate of occurrence, versus using a rate from some other published source. 


The model for the mean of the outcome would look like this:

$$log(y)= \beta_0 + \beta_1 x_1  + log(E)$$.



### Binomial model for counts 
You have probably seen the binomial distribution in either a basic statistics course, remember the coin flips? Or in the context of a logistic regression model. 

There are two ways the binomial distribution is typically used, the first is the context of logistic regression, where a special case of the binomial is used, called the **_Bernoulli_** distribution. This is the case of the binomial when there is basically a single coin flip, and you're trying to figure out the probability that it is heads (or tails). This is said to be a single **_trial_**, and the outcome is either 1 or 0 (heads or tails). 

The second way the binomial is used is when you have multiple trials, and you're trying to estimate the probability of the event occurring over multiple trials. In this case, your number of trials, $n$ can be large, and your number of successes, $y$ is the random variable under consideration. This is the basic makeup of a demographic rate, the count-based binomial.

The mean of the binomial distribution is a proportion or a probability, $\pi$, which tells you how likely it is that the event your interested in occurs. Any model using the binomial distributor will be geared towards estimating the probability. 

When a variable is coded as binary, meaning either 1 or 0, the Bernoulli distribution is used, as in the logistic regression model. When coded like this, the model tries to use the other measured information to predict the 1 value versus the 0 value. So in the basic sense, we want to construct a model like:

$$Pr(y=1) =\pi =  \text{some function of predictors}$$

The good thing is that, when we have count data, not just 1's and 0's, the same thing happens. The ratio or successes ($y$) to trials ($n$) is used to estimate $\pi$ and we build a model for that rate:

$$\text{Binomial} \binom{n}{y} = \frac{y}{n} = \pi = \text{some function of predictors}$$  



### Binomial regression models 
The ratio  $\frac{y}{n}$ is a rate or probability, and as such has very strict bounds. Probabilities cannot be less than 0 or greater than 1, so again, we should not use the Normal distribution here, since it is valid for all real numbers. Instead, we are using the binomial, but we still run into the problem of having a strictly bounded value, $\pi$ that we are trying to estimate with a linear function. 

Enter the link function again. 

The binomial distribution typically uses either a [logit](https://en.wikipedia.org/wiki/Logit) or [probit](https://en.wikipedia.org/wiki/Probit) link function, but others such as the [complementary log-log link function](http://data.princeton.edu/wws509/notes/c3s7.html) are also used in certain circumstances. For now we will use the logit function. 

The logit transforms the probability, $\pi$, which is bound on  the interval $[0,1]$ into a new limitless interval similar to the normal distribution of $[-\infty, \infty]$. The transformation is knows a the log-odds transformation, or logit for short. 

The odds of an event happening are the probability that something happens, divided by the probability it does not happen, in this case:

$$\text{odds }{\pi} = \frac{\pi}{(1-\pi)}$$

Which is bound on the interval $[0, \infty]$, when we take the natural log of the odds, the value is transformed into the linear space, of $[-\infty, \infty]$.

$$\text{log-odds }{\pi} = log  \left ( \frac{\pi}{(1-\pi)}  \right) $$

This can be modeled using a linear function of covariates now, without worrying about the original boundary problem:

$$log  \left ( \frac{\pi}{(1-\pi)}  \right) = \beta_0 +\beta_1 x_1$$
or more compactly:
$$log it(\pi)  = \beta_0 +\beta_1 x_1$$

## Interpretation of the binomial model 
Similar to when the Poisson model was introduced, the binomial model also has a strange interpretation when compared to the OLS model. 

Since we used the log-odds, or logit transformation of the mean of the outcome, $\pi$, the interpretations of the model $\beta$'s are not on a linear scale, they are on a log-odds scale. While we can certainly interpret a positive $\beta$ as increasing the odds of $y$ occurring, or increasing the rate, and a negative $\beta$, as decreasing the odds, this is not commonly how the model effects are interpreted. 

Instead, the common interpretation when using the logit model is the **_odds ratio_** interpretation of $\beta$. To obtain the odds ratio, you must exponentiate the $\beta$ from the model. For example, a $\beta$ of 0.025 would be $\text{exp}(\beta) = 1.025$. The interpretation of this is similar to the Poisson model, and we would say that for a 1 unit increase in $x_1$, the odds of $y$ occurring are 2.5% higher. This percentage is obtained by subtracting 1 from the $\text{exp}(\beta)$, or $\text{% change in odds} = \text{exp}(\beta) - 1$, which in this case is 0.025. 

For a $\beta_1$ that is negative, such as $\beta_1 = -.15$, the exponent is 0.86. Following the rule we just used, we see that a one unit change in $x_1$ leads to a $\text{exp}(-0.15) - 1 = -.14$, or a 14% decrease in the odds of y occurring. 


## Area Resource File

While there are many sources of aggregate data, such as the CDC Wonder site, US Census and a plethora of state agencies, a good national level data resource, which focuses primarily on healthcare access, it the [Health Resource and Service Administration (HRSA)](https://www.hrsa.gov/) [Area Health Resource Files](https://data.hrsa.gov/topics/health-workforce/ahrf). 

This data source is published each year, since the mid 1990's and is publicly available. It uses US Counties as its level of data availability. Google Scholar has more than [3,100 citations](https://scholar.google.com/scholar?q=%22area+resource+file%22&hl=en&as_sdt=0%2C44&as_ylo=2005&as_yhi=) of this data source since 2005, from a variety of disciplines. It has been used for many demographic studies of health and mortality as well. 

The data contain a wealth of information on healthcare access and availability, but also large sets of information on basic population counts, Census data and data from the National Center for Health Statistics and the vital registration system. 

We will use these data to provide examples of using count data models.

## Applications of count data modeling  
Below we will load the 2017-2018 Area Resource File from github and create rename some variables. The data have 7,277 variables for 3,230 county geographies. 

```{r}
arf<-"https://github.com/coreysparks/data/blob/master/arf2018.Rdata?raw=true"
load(url(arf))

library(spdep)
library(MASS)
library(spatialreg)
library(tidyverse)
library(ggplot2)
```

For this example, we will use the number of low-birth weight births for the three year period, 2014 to 2016. Generally the ARF does not provide annual measures of vital information, instead the provide period measures over 3 or 5 year periods. 

For the names of the variables, consult the [data dictionary](https://data.hrsa.gov//DataDownload/AHRF/AHRF_USER_TECH_2017-2018.zip) for the file, which is an Excel spreadsheet with the variable names and descriptions. Descriptions of the original data sources and codes are also available. 

For our analysis, we will use the number of low birth weight infants and the total number of births as our numerator and denominator. 


We also rename several variables that we will use as predictors for the analysis: child poverty rate from 2010, the rural urban continuum code from USDA, the primary healthcare shortage area code from 2010, and the per capital number of OB-GYN's in the county. Then we filter to have non-missing cases, which reduces the number of counties from 3,230 to 2,292.

```{r}
library(dplyr)
arf2018<-arf2018%>%
  mutate(cofips=f00004, 
         coname=f00010,
         state = f00011,
         births1416=f1254614,
         births0608=f1254608,
         lowbw1416=f1255314,
         lowbw0608=f1255308,
         childpov10= f1332210,
         rucc= as.factor(f0002013),
         hpsa10= as.factor(f0978710),
         obgyn10_pc= 1000*(f1168410/ f0453010) )%>%
 dplyr:: select(births1416, lowbw1416,births0608, lowbw0608,state, cofips, coname, childpov10, rucc, hpsa10, obgyn10_pc)%>%
  filter(complete.cases(.))%>%
  as.data.frame()


head(arf2018)
summary(arf2018)


```

Here, we do a basic map of the outcome variable, and see the highest rates of low birth weight births in the southern US.

```{r, results="hide"}

library(tigris)
library(sf)
library(ggplot2)

options(tigris_class="sf")
usco<-counties(cb=T, year=2010)
usco$cofips<-substr(usco$GEO_ID, 10, 15)
sts<-states(cb = T, year=2010)
sts<-st_boundary(sts)%>%
  filter(!STATE %in% c("02", "15", "60", "66", "69", "72", "78"))

arf2018_m<-geo_join(usco, arf2018, by_sp="cofips", by_df="cofips",how="left" )


arf2018_m%>%
  filter(!STATE %in% c("02", "15", "60", "66", "69", "72", "78"))%>%
  mutate(lbrate=lowbw1416/births1416)%>%
  mutate(lb_group = cut(lbrate, breaks=quantile(lbrate, p=seq(0,1,length.out = 6), na.rm=T ), include.lowest=T ))%>%
  ggplot()+
  geom_sf(aes(fill=lb_group, color=NA))+
  scale_color_brewer(palette = "Blues")+
  scale_fill_brewer(palette = "Blues",na.value = "grey50")+
  geom_sf(data=sts, color="black")+
  coord_sf(crs = 2163)+
  ggtitle(label = "Proportion of births that were low birthweight, 2014-2016")

```



Now we use the two model, the Poisson and the Binomial to estimate regression models for our low birth weight outcome.

First, we will us the Poisson model. To include the offset term for the number of births in each county, we use the `offset()` function within the model.

In this model, we use the `hpsa10` variable as the independent variable. This variable indicates whether a county has a shortage of primary care doctors, it has three levels, 0 = not a shortage are, 1= the whole county is a shortage are, and 2= part of the county is a shortage area.


The model indicates that counties that are either partial or total shortage areas have higher low birth weight rates. 
```{r}

arf2018sub<-filter(arf2018_m, is.na(lowbw1416)==F)


fit_pois<- glm(lowbw1416 ~ offset(log(births1416+.00001)) + hpsa10, 
               family=poisson, 
               data=arf2018sub)
summary(fit_pois)

```


If we exponentiate the coefficients from the model, we can get the risk ratios:

```{r}

exp(coef(fit_pois))

```

In this case, the `hpsa10, 1`, or whole shortage area counties, have a mean low birth-weight rate that is 3.8% higher than counties that are not shortage areas, and counties that are partial shortage areas, `hpsa10, 2` have a mean low birth weight rate that is 4.3% higher. 

In practical terms, we can calculate these rates ourselves, after extracting the estimated counts of low birth weight births:

```{r}
arf2018sub$est_pois<- fitted(fit_pois)

arf2018sub$est_rate<- arf2018sub$est_pois/ arf2018sub$births1416

aggregate(est_rate ~ hpsa10, data=arf2018sub, FUN = mean)

```

We can show through arithmetic that the difference between `hpsa10` level 0 and `hpsa10` level 1 is 3.8% and the difference between the `hpsa10` level 0 and `hpsa10` level 2 is 4.4%

```{r}
(0.08092246 - 0.07791448)/0.07791448
(0.08140300 - 0.07791448)/0.07791448
```

## Issues with the Poisson model

The Poisson distribution model has a strong assumption to it, the assumption is that the mean and variance of the Poisson model are the same. If the variance is greater than the mean, then the model has an **_overdispersion_** problem, meaning the variance is greater than the mean. 

There is a general test to examine this in a Poisson model, and it relies on the model fit statistics generated by the`summary()` function.

Two numbers in the model summary, the residual deviance and the residual degrees of freedom. If the Poisson model is fitting the data appropriately, then these two values will be the same, or very similar. As the residual deviance becomes larger than the residual degrees of freedom, the model becomes more over dispersed. 

A check of this is to examine the ratio of these to values:

```{r}
scale<-sqrt(fit_pois$deviance/fit_pois$df.residual)
scale

```

This value should be 1 if the mean and variance are equal, but in this case it is `r round (scale, 2)`. This suggests that the variance is twice as large as the mean. 

There is also a test statistic that can be used, referred to as a **_goodness of fit statistic_**. This compares the model deviance to a $\chi^2$ distribution with degrees of freedom equal to the residual degrees of freedom. Small p-values for this test, suggest the model does not fit the data. 

```{r}
1-pchisq(fit_pois$deviance, df = fit_pois$df.residual)
```

In this case, the statistic has a p-value of 0, indicating the model does not fit the data well at all. 

When overdispersion is present in a model, the model results cannot be trusted, because the test statistics of the model rely on the assumptions to be correct. 


### Changing the model 
When overdispersion is detected, the easiest thing to do is use a different model. One option is to use a **_quasi-Poisson_** model

For the Poisson, the assumption of the basic model is :

$$var(Y) = \lambda $$

the quasi-Poisson model includes an **_overdispersion parameter_**, which scales the variance of the model, so the model assumption becomes

$$var(Y) = \lambda * \phi$$

This allows us to include a proxy for a dispersion parameter for the distribution. Naturally this is fixed at 1 for basic models, and estimated in the quasi models, we can look to see if is much bigger than 1.

This can be done in R:

```{r}
fit_q_pois<- glm(lowbw1416 ~ offset(log(births1416)) + hpsa10, 
               family=quasipoisson, 
               data=arf2018sub)
summary(fit_q_pois)


```

This output shows that the new test statistics are approximately half as large as they were under the Poisson assumptions, and the dispersion parameter is 4.3, in the regular Poisson model, it was assumed to be 1. 

While the substantive differences of the two models are the same, the quasi-Poisson model should be used in this case. 

## More alternatives to the Poisson 

Another general option if the Poisson model shows overdispersion, is to use a different model!  In other words, if the Poisson distribution does not fit, then use a different distribution. A natural alternative to the Poisson distribution is the **_Negative Binomial distribution_**, not to be confused with the Binomial distribution we have already described. 

The Negative Binomial distribution is also appropriate for count data, but unlike the Poisson distribution, it includes a second parameter in its distribution function that allows the variance to scale automatically with the mean, basically what the quasi-Poisson model was doing, but unlike the quasi distribution, the Negative Binomial distribution has a true likelihood function. 

You can get the Negative Binomial distribution model in the `MASS` library that comes with R. It is used in the same way as the Poisson model, but using the `glm.nb()` function. 

```{r}
library(MASS)
fit_nb<- glm.nb(lowbw1416 ~ offset(log(births1416)) + hpsa10, 
               data=arf2018sub)
summary(fit_nb)


```

Again, we see the same overall interpretation of the model effects, but the risk ratios are different compared to the Poisson model:

```{r}

exp(coef(fit_nb))

```
The Negative Binomial model shows even higher levels of relative risk in the undeserved areas, with a 6.35% increase in risk for counties are totally under-served, and 5.16% difference for counties that are partially under-served. This actually makes more sense that the Poisson model results:

```{r}

exp(coef(fit_pois))

```

which showed higher risk in the partially under-served areas. 

## Relative risk model

Earlier in the lesson, the epidemiological concept of relative risk was introduced, as was the concept of the expected number of cases. To calculate the expected number of low birth weight births, we can calculate the national low birth weight rate and multiply it by the number of births in each county:

```{r}

lbrate<-(sum(arf2018sub$lowbw1416, na.rm=T)/sum(arf2018sub$births1416, na.rm=T)) 
lbrate

```

Which estimates that on average, 8.07% of births should be low birth weight. We can apply this rate to each county's births by:

```{r}
arf2018sub$E <- lbrate* arf2018sub$births1416

head(arf2018sub[, c("coname", "births1416", "lowbw1416", "E")])
```

This shows us the observed number of low birth weight births, and the expected number. These are usually pretty similar. You can calculate the **_relative risk_** as $RR = \frac{y}{E}$, below we compare the observed number of births to the standardized number. 

The first plot basically shows that the distribution is very right- skewed, and is conflated with the overall number of births in the county. While the second plot control for the number of births in the county through the expected value, and the distribution is centered around 1. 

We see that some counties have higher than expected risk $\frac{y}{E} >1$  and some counties have lower risk, $\frac{y}{E} <1$. Often times, the goal of population epidemiology is to identify factors that are related to patterns of excess risk. 

```{r}

arf2018sub%>%
  ggplot( aes(lowbw1416))+geom_histogram()+ggtitle("Distribution of low birthweight births", "y")

arf2018sub%>%
  ggplot( aes(lowbw1416/E))+geom_histogram()+ggtitle("Distribution of standardized low birthweight births", "y/E")


```

In order to estimate the model with the expected counts, you change the offset term in the model, otherwise everything is the same:
```{r}

fit_pois_E<-glm(lowbw1416 ~ offset(log(E)) + hpsa10, 
               family=poisson, 
               data=arf2018sub)
summary(fit_pois_E)
```

In fact, these results are identical to those from the Poisson model with the births as the offset term.

## Binomial count model

The binomial model described earlier in the lesson can also be fit with the `glm()` function. In order to identify the numerator and the denominator, we have to specify the outcome in the model a little differently:

```{r}

fit_bin<-glm(lowbw1416~  hpsa10, weights = births1416,
               family=binomial, 
               data=arf2018sub)
summary(fit_bin)

```

In this model, we see the exact same results compared to the Poisson model. The only difference is the AIC for the binomial model is substantially lower, suggesting that that distribution is preferred in this analysis. 

## Filtering GLMs for spatial autocorrelation

### Autocorrelation in GLMs
While we don't have the same assumption about residuals for GLMs as we do for the Gaussian (OLS) model, we still may be curious if our model's residuals display spatial clustering. While it's not [recommended](https://stat.ethz.ch/pipermail/r-sig-geo/2009-November/007034.html), we can use the `lm.morantest()` function to examine correlation among the residuals.


```{r,fig.width=9, fig.height=8}

library(spdep)
nbs<-knearneigh(coordinates(as(arf2018sub, "Spatial")), k = 4, longlat = T)
nbs<-knn2nb(nbs, sym = T)
us.wt4<-nb2listw(nbs, style = "W")
lm.morantest(fit_pois, listw = us.wt4)
lm.morantest(fit_bin, listw = us.wt4)
lm.morantest(fit_nb, listw = us.wt4)

```

### Spatial nuisance and GLMs
Inherently, we are kind of doing spatial analysis since we are using areal units as our observations, right Paul? There are other options that have been explored that promote not modeling space explicityly but instead filtering out the nuisance of spatial autocorrelation. Spatial Filtering is a method Griffith and Peres-Neto [(2006)](http://www.esajournals.org/doi/abs/10.1890/0012-9658(2006)87%5B2603:SMIETF%5D2.0.CO%3B2) and Dray S, Legendre P and Peres-Neto PR [(2005)](http://www.sciencedirect.com/science/article/pii/S0304380006000925) both describe this method. The Moran eigenvector filtering function is intended to remove spatial autocorrelation from the residuals of (generalised) linear models. Tiefelsdorf M, Griffith DA. [(2007)](http://www.spatialfiltering.com/) also describe a method of using eigenvectors of the spatial weight matrix to filter out autocorrelation in a model. 

Typically this is done by 1) forming the spatial weight matrix, 2)compute the eigenvectors of the centered weight matrix: 
$$(I-1 1 ' /n)W (I-1 1 ' /n)$$,
where **I** is the identity matrix, and **1** is a vector of 1's. 3) select the eigenvectors to be included in a linear model as spatial predictors, so that the level of autocorrelation in the model is minimized, until Moran's I is nonsigificant. We can do this using the `ME()` in the `spdep` library.

`ME()` example:

```{r,fig.width=9, fig.height=8}
#you have to provide the theta value for the NB family, I got this from the model fit above (fitnb).

arf2018sub<-arf2018sub%>%
  filter(!STATEFP %in% c("02", "15", "60", "66", "69", "72", "78"))

nbs<-knearneigh(coordinates(as(arf2018sub, "Spatial")), k = 4, longlat = T)
nbs<-knn2nb(nbs, sym = T)
us.wt4<-nb2listw(nbs, style = "W")

fitnb<-glm.nb(lowbw1416 ~ offset(log(E)) + hpsa10 + I(rucc%in%c("01", "02", "03"))+ childpov10,
           data=arf2018sub)

summary(fitnb)
fitnb$theta
me.fit<-ME(lowbw1416 ~ offset(log(E)) + hpsa10 + I(rucc%in%c("01", "02", "03"))+ childpov10,
           data=arf2018sub,
           family=negative.binomial(74.75),
           listw = us.wt4,
           verbose=T,alpha=.1 )
me.fit

fits<-data.frame(fitted(me.fit))
arf2018sub$me1<-fits$vec1
arf2018sub$me10<-fits$vec10
```

```{r }
arf2018sub%>%
  ggplot()+
  geom_sf(aes(fill=me1))+
  scale_fill_viridis_c()+
  coord_sf(crs = 2163)+
  ggtitle("First Moran Eigenvector")

```

```{r}


arf2018sub%>%
  ggplot()+
  geom_sf(aes(fill=me10))+
  scale_fill_viridis_c()+
  coord_sf(crs = 2163)+
  ggtitle("Moran Eigenvector 10")

```

Then I plug the scores of each county on these Moran eigenvectors into the model to "clean" it:

```{r}
clean.nb<-glm.nb(lowbw1416 ~ offset(log(E)) + hpsa10 + I(rucc%in%c("01", "02", "03"))+ childpov10+fitted(me.fit), arf2018sub)
summary(clean.nb)

lm.morantest(fitnb, listw=us.wt4)

lm.morantest(clean.nb, listw=us.wt4)

```

So we decrease the autocorrelation in the model, but not entirely.


### Auto covariate-models
One model that has been proposed is the auto - model. This includes an auto-covariate in the model, which is an average of the neighboring values outcome: 

$$log(y) = X ' \beta + W y$$.

This can be used for binomial or Poisson but people don't really seem to like this. 

```{r,fig.width=9, fig.height=9}
#Creating an auto- model
#this involves creating an "auto-covariate"
arf2018sub$lag_rate<-lag.listw(x=us.wt4, var=(arf2018sub$lowbw1416/arf2018sub$births1416))

fit.nb.auto<-glm.nb(lowbw1416 ~ offset(log(E)) +lag_rate+ hpsa10 + I(rucc%in%c("01", "02", "03"))+ childpov10, 
                     data=arf2018sub)

lm.morantest(fit.nb.auto, listw=us.wt4)
summary(fit.nb.auto)

AIC(fitnb)
AIC(clean.nb)
AIC(fit.nb.auto)

```

## Lesson Wrap up 
In this lesson, we saw how the generalized linear model framework can be applied to count data at the aggregate level in spatial demographic research. Three commonly used count-data distributions were reviewed and shown to produce similar results. 

These types of models are often better suited than the OLS or other linear regression model that assumes the Normal distribution, because the Normal model can often produce estimates that are outside of the range of demographic rates. 

## Coding practice
To further practice the principles from this lesson, we will use the Area Resource File again. This time we will do an analysis that looks at the change in the rate of insurance before and after the Affordable Care Act, and if the difference is the same based off the type of place one lives.

To do this, we will need the population under age 65, and the population under age 65 who were insured, in two years. We will use 2010 and 2016. We also need the rural-urban continuum codes. 

```{r}
#load the data
arf<-"https://github.com/coreysparks/data/blob/master/arf2018.Rdata?raw=true"
load(url(arf))

library(dplyr)
library(ggplot2)
arf2018<-arf2018%>%
  mutate(cofips=f00004, 
         coname=f00010,
         state = f00011,
         pop_16= f1547116,
         pop_10=  f1474810 ,
         ins_16= f1547216,
         ins_10 = f1474910,
         rucc= as.factor(f0002013),
         hpsa10= as.factor(f0978710) )%>%
  dplyr::select(pop_10, pop_16,ins_10, ins_16,state, cofips, coname, rucc, hpsa10)%>%
  filter(complete.cases(.))%>%
  as.data.frame()


rem<-duplicated(arf2018$cofips)
arf2018<-arf2018[rem==F,]
arf_long<- reshape(data=arf2018, idvar  = "cofips",
               varying=list(names(arf2018)[c(1,2)],
                            names(arf2018)[c(3,4)]), 
               direction = "long",
               times=c(2010, 2016), 
              v.names=c("pop","insur"))

#descriptive plot of the %insured
arf_long%>%
  mutate(ins_rate=insur/pop)%>%
  ggplot()+geom_density(aes(ins_rate))+facet_wrap(~time)


library(MASS)
fit_nb<-glm.nb(insur~offset(log(pop))+factor(time)*rucc, data=arf_long)

summary(fit_nb)

newdat<-expand.grid(time=levels(factor(arf_long$time)), rucc=levels(factor(arf_long$rucc)), pop=quantile(arf_long$pop, p= .5))

#use the model to generate fitted values
newdat$fitted<-predict(fit_nb, newdata=newdat, type = "response")
newdat$ins_rate<-newdat$fitted/newdat$pop

newdat%>%
  ggplot(aes(x=time, y=ins_rate))+geom_bar(aes(fill = rucc), 
   width = 0.4, position = position_dodge(width=0.5), stat="identity")+
  scale_colour_viridis_d()+
  scale_fill_viridis_d()+ggtitle(label = "Estimated Insurance Coverage Rates by Rural Urban Continuum Code"
                                 , subtitle = "2010 and 2016")

```